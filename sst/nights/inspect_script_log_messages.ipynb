{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7762da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs\n",
    "ndays = \"4d\"  # Number of days/hours to query\n",
    "client_name = \"usdf_efd\"  # EFD client name\n",
    "path = \"auxtel/correct_pointing.py\"  # Script path\n",
    "salIndex = 2  # Script Queue Sal Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae187d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate input data\n",
    "\n",
    "# Validates that ndays is a string that can either a number of hours (e.g. '6h') or a number of days (e.g. '3d')\n",
    "if not ndays[-1] in [\"h\", \"d\"]:\n",
    "    raise ValueError(\"ndays must end in either 'h' or 'd'\")\n",
    "\n",
    "# Assert cliente name is either summit_efd or usdf_efd\n",
    "assert client_name in [\n",
    "    \"summit_efd\",\n",
    "    \"usdf_efd\",\n",
    "], \"client_name must be either 'summit_efd' or 'usdf_efd'\"\n",
    "\n",
    "# Assert path is a string\n",
    "assert isinstance(path, str), \"path must be a string\"\n",
    "\n",
    "# Assert salIndex is an integer either 1, 2 or 3\n",
    "assert salIndex in [1, 2, 3], \"salIndex must be an integer either 1, 2 or 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c764c4",
   "metadata": {},
   "source": [
    "# Script Log Inspection Notebook\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is designed to query and analyze script logs for specific executions.\n",
    "It allows users to inspect detailed log messages for a given script path and Script Queue (Sal Index), providing insights into script behavior, potential issues, and tracebacks.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "The inputs for this notebook are:\n",
    "\n",
    "1. **`ndays`:**\n",
    "   Allows selecting the number of days (**or hours**) to query the script queue logs (e.g., '1h', '10h', '0.5d', '1d', '10d', etc.).\n",
    "\n",
    "2. **`EFD Client`:**\n",
    "   Allows switching between available EFD clients (such as `usdf_efd` or `summit_efd`) if you need to query data from different environments.\n",
    "   If analyzing real-time data, the `summit_efd` might be the preferred client.\n",
    "\n",
    "3. **`Script Path`:**\n",
    "   Lets you select the script path for which you want to query the logs (e.g., `maintel/m1m3/check_actuators.py`, `maintel/track_target.py`, `auxtel/track_target.py`, etc.).\n",
    "\n",
    "4. **`SalIndex`:**\n",
    "   Lets you select the Script Queue `SalIndex` to query logs from different queues:\n",
    "   - `1`: Simonyi Telescope Queue\n",
    "   - `2`: AuxTel Queue\n",
    "   - `3`: OCS Queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d945a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and basic setup\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.time import Time, TimeDelta\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    ColumnDataSource,\n",
    "    CustomJS,\n",
    "    DataTable,\n",
    "    Div,\n",
    "    Select,\n",
    "    TableColumn,\n",
    ")\n",
    "from IPython.display import HTML, display\n",
    "from lsst_efd_client import EfdClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60054756",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "\n",
    "### Query and Filter Script Queue Logs\n",
    "\n",
    "\n",
    "# This function converts user-defined time ranges into hours for querying the EFD.\n",
    "def convert_to_hours(past_time):\n",
    "    \"\"\"\n",
    "    Convert a string that can be either a number of hours (e.g. '6h')\n",
    "    or a number of days (e.g. '3d') to total number of hours (int).\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d+)([dh]?)\", past_time)\n",
    "    if match:\n",
    "        value, unit = match.groups()\n",
    "        value = int(value)\n",
    "        if unit == \"d\":\n",
    "            return value * 24\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid time format. Please use a format \"\n",
    "            \"like '6h' for hours or '3d' for days.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# A function to retrieve script queue logs for a given time range and client.\n",
    "async def query_script_queue_logs(\n",
    "    start_time_str: str, end_time_str: str, client_name=\"usdf_efd\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Queries the log messages related to the script queue from the EFD.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_time_str : str\n",
    "        Start time in ISO format, e.g. \"2024-11-04T12:00:00\".\n",
    "    end_time_str : str\n",
    "        End time in ISO format, e.g. \"2024-11-04T13:00:00\".\n",
    "    client_name : str, optional\n",
    "        Name of the EFD client. Defaults to \"usdf_efd\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with script queue logs in the given time window.\n",
    "    \"\"\"\n",
    "    # Convert string times to astropy Time\n",
    "    start = Time(start_time_str, format=\"isot\", scale=\"utc\")\n",
    "    end = Time(end_time_str, format=\"isot\", scale=\"utc\")\n",
    "\n",
    "    # Create the EFD client\n",
    "    possible_clients = [\"summit_efd\", \"usdf_efd\"]\n",
    "    if client_name not in possible_clients:\n",
    "        print(f\"Invalid client name. Possible clients: {possible_clients}\")\n",
    "        return None\n",
    "\n",
    "    client = EfdClient(client_name)\n",
    "    script_logs = await client.select_time_series(\n",
    "        topic_name=\"lsst.sal.ScriptQueue.logevent_script\",\n",
    "        fields=\"*\",\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "\n",
    "    return script_logs\n",
    "\n",
    "\n",
    "# This function filters raw logs to focus on a specific script and processes the data into a clean format.\n",
    "def filter_and_process_queue_logs(\n",
    "    script_logs, path=\"maintel/m1m3/check_actuators.py\", salIndex=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters and processes the script logs DataFrame for a specific  path and salIndex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    script_logs : pd.DataFrame\n",
    "        DataFrame containing the script logs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Processed DataFrame containing only relevant logs for the check_actuators script.\n",
    "    \"\"\"\n",
    "    processState_mapping = {\n",
    "        0: \"UNKNOWN\",\n",
    "        1: \"LOADING\",\n",
    "        2: \"CONFIGURED\",\n",
    "        3: \"RUNNING\",\n",
    "        4: \"DONE\",\n",
    "        5: \"LOADFAILED\",\n",
    "        6: \"CONFIGURE_FAILED\",\n",
    "        7: \"TERMINATED\",\n",
    "    }\n",
    "\n",
    "    scriptState_mapping = {\n",
    "        0: \"UNKNOWN\",\n",
    "        1: \"UNCONFIGURED\",\n",
    "        2: \"CONFIGURED\",\n",
    "        3: \"RUNNING\",\n",
    "        4: \"PAUSED\",\n",
    "        5: \"ENDING\",\n",
    "        6: \"STOPPING\",\n",
    "        7: \"FAILING\",\n",
    "        8: \"DONE\",\n",
    "        9: \"STOPPED\",\n",
    "        10: \"FAILED\",\n",
    "        11: \"CONFIGURE_FAILED\",\n",
    "    }\n",
    "\n",
    "    df_script_logs = pd.DataFrame(script_logs)\n",
    "\n",
    "    # Filter logs for the specific script path and salIndex\n",
    "    df_filtered_logs = df_script_logs[\n",
    "        (df_script_logs[\"path\"] == path) & (df_script_logs[\"salIndex\"] == salIndex)\n",
    "    ]\n",
    "\n",
    "    # Reindex by private_rcvStamp, convert to UTC\n",
    "    df_filtered_logs = df_filtered_logs.set_index(\"private_rcvStamp\")\n",
    "    df_filtered_logs.index = pd.to_datetime(df_filtered_logs.index, unit=\"s\")\n",
    "    df_filtered_logs.index = df_filtered_logs.index - timedelta(seconds=37)\n",
    "\n",
    "    # Convert all columns starting with 'timestamp' to datetime, also shift TAI->UTC\n",
    "    timestamp_columns = [\n",
    "        col for col in df_filtered_logs.columns if col.startswith(\"timestamp\")\n",
    "    ]\n",
    "    for col in timestamp_columns:\n",
    "        df_filtered_logs[col] = pd.to_datetime(df_filtered_logs[col], unit=\"s\")\n",
    "        df_filtered_logs[col] = df_filtered_logs[col] - timedelta(seconds=37)\n",
    "\n",
    "    # Map numeric process/script states to strings\n",
    "    df_filtered_logs[\"processState_str\"] = df_filtered_logs[\"processState\"].map(\n",
    "        processState_mapping\n",
    "    )\n",
    "    df_filtered_logs[\"scriptState_str\"] = df_filtered_logs[\"scriptState\"].map(\n",
    "        scriptState_mapping\n",
    "    )\n",
    "\n",
    "    # Remove unneeded columns\n",
    "    columns_to_remove = [\n",
    "        \"blockId\",\n",
    "        \"blockSize\",\n",
    "        \"cmdId\",\n",
    "        \"executionId\",\n",
    "        \"private_efdStamp\",\n",
    "        \"private_kafkaStamp\",\n",
    "        \"private_origin\",\n",
    "        \"private_revCode\",\n",
    "        \"private_sndStamp\",\n",
    "        \"private_seqNum\",\n",
    "        \"scriptBlockIndex\",\n",
    "        \"private_identity\",\n",
    "        \"processState\",\n",
    "        \"scriptState\",\n",
    "    ]\n",
    "    df_filtered_logs.drop(columns=columns_to_remove, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Sort from most recent to oldest\n",
    "    df_filtered_logs.sort_index(ascending=False, inplace=True)\n",
    "\n",
    "    return df_filtered_logs\n",
    "\n",
    "\n",
    "# Extracts details such as start time, end time, and final status for each execution.\n",
    "def extract_execution_details(df_check_actuators_log):\n",
    "    \"\"\"\n",
    "    Extracts execution details from the script logs DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_check_actuators_log : pd.DataFrame\n",
    "        DataFrame containing the filtered script logs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing execution times, durations, and final statuses.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store execution details\n",
    "    execution_data = []\n",
    "\n",
    "    # Loop over each unique scriptSalIndex\n",
    "    for script_sal_index in df_check_actuators_log[\"scriptSalIndex\"].unique():\n",
    "        df_sal = df_check_actuators_log[\n",
    "            df_check_actuators_log[\"scriptSalIndex\"] == script_sal_index\n",
    "        ]\n",
    "        df_sal = df_sal.sort_index()  # Ensure chronological order\n",
    "\n",
    "        # Calculate start and end times for each execution\n",
    "        start_time = df_sal[\"timestampProcessStart\"].min()\n",
    "        end_time = df_sal[\"timestampProcessEnd\"].max()\n",
    "\n",
    "        # Get the final process and script status\n",
    "        final_process_status = df_sal[\"processState_str\"].iloc[-1]\n",
    "        final_script_status = df_sal[\"scriptState_str\"].iloc[-1]\n",
    "\n",
    "        execution_data.append(\n",
    "            {\n",
    "                \"path\": df_sal[\"path\"].iloc[0],\n",
    "                \"scriptSalIndex\": script_sal_index,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"FinalProcessStatus\": final_process_status,\n",
    "                \"FinalScriptStatus\": final_script_status,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df_executions = pd.DataFrame(execution_data)\n",
    "\n",
    "    if not df_executions.empty:\n",
    "        # Calculate duration in minutes\n",
    "        df_executions[\"duration_minutes\"] = (\n",
    "            df_executions[\"end_time\"] - df_executions[\"start_time\"]\n",
    "        ).dt.total_seconds() / 60.0\n",
    "\n",
    "        # Format durations to .2f\n",
    "        df_executions[\"duration_minutes\"] = df_executions[\"duration_minutes\"].apply(\n",
    "            lambda x: \"{:.2f}\".format(x)\n",
    "        )\n",
    "\n",
    "        # Reorder columns\n",
    "        cols = [\n",
    "            \"path\",\n",
    "            \"scriptSalIndex\",\n",
    "            \"start_time\",\n",
    "            \"end_time\",\n",
    "            \"duration_minutes\",\n",
    "            \"FinalProcessStatus\",\n",
    "            \"FinalScriptStatus\",\n",
    "        ]\n",
    "        df_executions = df_executions[cols]\n",
    "\n",
    "        # Sort by start time in descending order\n",
    "        df_executions = df_executions.sort_values(\"start_time\", ascending=False)\n",
    "    else:\n",
    "        print(\"No executions found.\")\n",
    "\n",
    "    return df_executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba825ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters Review\n",
    "\n",
    "#### This cell prints the user-defined parameters to ensure correctness before querying.\n",
    "\n",
    "print(\"Parameters set:\")\n",
    "print(f\"  Number of days/hours: {ndays}\")\n",
    "print(f\"  EFD Client: {client_name}\")\n",
    "print(f\"  Sal Index: {salIndex}\")\n",
    "print(f\"  Script Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808a238",
   "metadata": {},
   "source": [
    "## Query Script Executions\n",
    "\n",
    "This section queries the EFD for script executions, processes the results and displays details for the executions found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_hours = convert_to_hours(ndays)\n",
    "\n",
    "now_utc = datetime.utcnow()\n",
    "end_dt = now_utc\n",
    "start_dt = now_utc - timedelta(hours=past_hours)\n",
    "\n",
    "start_str = start_dt.isoformat()\n",
    "end_str = end_dt.isoformat()\n",
    "\n",
    "print(f\"Querying script executions for {path} from {start_str} to {end_str}...\")\n",
    "script_logs = await query_script_queue_logs(start_str, end_str, client_name)\n",
    "\n",
    "if script_logs.empty:\n",
    "    print(\"No script executions found.\")\n",
    "else:\n",
    "    df_logs = filter_and_process_queue_logs(script_logs, path=path, salIndex=salIndex)\n",
    "    df_executions = extract_execution_details(df_logs)\n",
    "\n",
    "# Display executions summary\n",
    "if df_executions.empty:\n",
    "    raise ValueError(\n",
    "        \"No executions found in the last {:.1f} days.\".format(past_hours / 24.0)\n",
    "    )\n",
    "else:\n",
    "    print(\"Executions found in the last {:.1f} days:\".format(past_hours / 24.0))\n",
    "    display(HTML(df_executions.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791396a6",
   "metadata": {},
   "source": [
    "### Interactive Log Inspection App\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "- **Dropdown:**\n",
    "   Select a specific Script Queue `SalIndex` to inspect logs for executions.\n",
    "\n",
    "- **Log Messages Table:**\n",
    "   Displays a tabular view of the log messages for the selected execution, including columns for `filePath`, `functionName`, `message`, and truncated `traceback`.\n",
    "\n",
    "- **Traceback Viewer:**\n",
    "   Clicking on a row in the table shows the full traceback in a separate viewer if available.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Upon selecting a specific `SalIndex` and interacting with the log inspection table:\n",
    "\n",
    "1. **`Log Messages Table`:**\n",
    "   - A tabular format showing the log fields, such as `filePath`, `functionName`, `message`, and `traceback`.\n",
    "   - Allows row selection to view full tracebacks.\n",
    "\n",
    "2. **`Traceback Viewer`:**\n",
    "   - Displays the full traceback for the selected log message.\n",
    "   - If no traceback is present, the viewer indicates this to the user.\n",
    "\n",
    "This interactive app makes it easy to explore and analyze script logs, facilitating efficient debugging and understanding of script behaviors across different queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to truncate a traceback string if it's too long.\n",
    "def truncate_traceback(tb_str, length=120):\n",
    "    \"\"\"\n",
    "    Return a truncated version of tb_str if it's longer than 'length'.\n",
    "    Append '... [click row for full]' at the end to signal more content.\n",
    "    \"\"\"\n",
    "    if not tb_str:\n",
    "        return \"\"\n",
    "    tb_str = str(tb_str)  # Ensure it's a string\n",
    "    if len(tb_str) <= length:\n",
    "        return tb_str\n",
    "    else:\n",
    "        return tb_str[:length] + \"... [click row for full]\"\n",
    "\n",
    "\n",
    "# A function to build log data for each execution by querying the SAL log messages.\n",
    "async def build_log_data_by_salindex(df_executions, client_name=\"usdf_efd\"):\n",
    "    \"\"\"\n",
    "    For each row in df_executions, query lsst.sal.Script.logevent_logMessage\n",
    "    in the time range [start_time, end_time], and build a dictionary\n",
    "    mapping salIndex -> {filePath, functionName, message, traceback}.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_executions : pd.DataFrame\n",
    "        Must include columns \"scriptSalIndex\", \"start_time\", and \"end_time\".\n",
    "    client_name : str\n",
    "        EFD client name (\"usdf_efd\", \"summit_efd\", etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary keyed by SAL index (str), with values as column-data dict:\n",
    "        {\n",
    "            \"filePath\":     [...],\n",
    "            \"functionName\": [...],\n",
    "            \"message\":      [...],\n",
    "            \"traceback\":    [...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Create an EFD client\n",
    "    client = EfdClient(client_name)\n",
    "\n",
    "    # Prepare a dict to store all log messages, keyed by SAL index\n",
    "    log_data = {}\n",
    "\n",
    "    for _, row in df_executions.iterrows():\n",
    "        sal_idx = row[\"scriptSalIndex\"]\n",
    "        start_time = row[\"start_time\"]\n",
    "        end_time = row[\"end_time\"]\n",
    "\n",
    "        # Convert to ISO strings if needed\n",
    "        start_str = start_time.isoformat()\n",
    "        end_str = end_time.isoformat()\n",
    "\n",
    "        # Query the 'Script.logevent_logMessage' topic for [start_time, end_time]\n",
    "        log_msgs = await client.select_time_series(\n",
    "            \"lsst.sal.Script.logevent_logMessage\",\n",
    "            fields=\"*\",\n",
    "            start=Time(start_str, format=\"isot\", scale=\"utc\"),\n",
    "            end=Time(end_str, format=\"isot\", scale=\"utc\"),\n",
    "        )\n",
    "\n",
    "        # If empty, store an empty dictionary structure\n",
    "        if len(log_msgs) == 0:\n",
    "            log_data[str(sal_idx)] = {\n",
    "                \"filePath\": [],\n",
    "                \"functionName\": [],\n",
    "                \"message\": [],\n",
    "                \"traceback\": [],\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Convert results to DataFrame, remove extraneous columns\n",
    "        df_msgs = pd.DataFrame(log_msgs)\n",
    "        drop_cols = [\n",
    "            \"private_efdStamp\",\n",
    "            \"private_identity\",\n",
    "            \"private_kafkaStamp\",\n",
    "            \"private_origin\",\n",
    "            \"private_rcvStamp\",\n",
    "            \"private_revCode\",\n",
    "            \"private_seqNum\",\n",
    "            \"private_sndStamp\",\n",
    "            \"process\",\n",
    "            \"salIndex\",\n",
    "            \"timestamp\",\n",
    "        ]\n",
    "        df_msgs.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # Ensure the required columns exist (some logs might be missing them)\n",
    "        for needed in [\"filePath\", \"functionName\", \"message\", \"traceback\"]:\n",
    "            if needed not in df_msgs.columns:\n",
    "                df_msgs[needed] = \"\"\n",
    "\n",
    "        # Create a truncated traceback column\n",
    "        df_msgs[\"tracebackFull\"] = df_msgs[\"traceback\"]\n",
    "        df_msgs[\"traceback\"] = df_msgs[\"tracebackFull\"].apply(truncate_traceback)\n",
    "\n",
    "        # Restrict to final columns\n",
    "        df_msgs = df_msgs[\n",
    "            [\"filePath\", \"functionName\", \"message\", \"traceback\", \"tracebackFull\"]\n",
    "        ]\n",
    "\n",
    "        # Convert to dict-of-lists\n",
    "        log_data[str(sal_idx)] = df_msgs.to_dict(orient=\"list\")\n",
    "\n",
    "    return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Complete Bokeh App Cell\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# 1) Activate Bokeh in the notebook\n",
    "output_notebook()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Build the dictionary log_data_by_salindex using your existing df_executions\n",
    "#    and the two helper functions you've provided:\n",
    "#    (a) truncate_traceback(tb_str, length=120)\n",
    "#    (b) build_log_data_by_salindex(df_executions, client_name=\"usdf_efd\")\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Check that df_executions is not empty\n",
    "if df_executions.empty:\n",
    "    raise ValueError(\n",
    "        \"No executions found. You may need to change notebook input parameters.\"\n",
    "    )\n",
    "\n",
    "# Build the log data dictionary\n",
    "log_data_by_salindex = await build_log_data_by_salindex(df_executions, client_name)\n",
    "\n",
    "# 3) Pick the first SAL index in df_executions as our initial selection.\n",
    "initial_salindex = df_executions[\"scriptSalIndex\"].iloc[0]\n",
    "\n",
    "# Grab the corresponding data dict (or an empty fallback).\n",
    "initial_data = log_data_by_salindex.get(\n",
    "    str(initial_salindex),\n",
    "    {\n",
    "        \"filePath\": [],\n",
    "        \"functionName\": [],\n",
    "        \"message\": [],\n",
    "        \"traceback\": [],\n",
    "        \"tracebackFull\": [],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the ColumnDataSource with that initial data\n",
    "source = ColumnDataSource(initial_data)\n",
    "\n",
    "# 4) Define the columns for the main DataTable.\n",
    "#    We show the truncated traceback in \"traceback\" (already truncated in the dict).\n",
    "columns = [\n",
    "    TableColumn(field=\"filePath\", title=\"filePath\", width=600),\n",
    "    TableColumn(field=\"functionName\", title=\"functionName\", width=200),\n",
    "    TableColumn(field=\"message\", title=\"message\", width=700),\n",
    "    TableColumn(field=\"traceback\", title=\"traceback\", width=200),\n",
    "]\n",
    "\n",
    "data_table = DataTable(\n",
    "    source=source,\n",
    "    columns=columns,\n",
    "    width=1700,\n",
    "    height=1200,\n",
    "    # This determines how row selection is made:\n",
    "    #   \"checkbox\" -> user must tick a box at left to select the row\n",
    "    #   \"row\" -> user clicks on the row\n",
    "    selectable=\"checkbox\",\n",
    "    # Hide the default numerical index column on the left\n",
    "    index_position=None,\n",
    ")\n",
    "\n",
    "# 5) Create a Div to display the FULL traceback text\n",
    "traceback_div = Div(\n",
    "    text=\"Select a row to see its full traceback here, if the traceback is not empty...\",\n",
    "    width=1700,\n",
    "    height=600,\n",
    "    styles={\"overflow\": \"auto\", \"border\": \"2px solid gray\", \"padding\": \"5px\"},\n",
    ")\n",
    "\n",
    "# 6) Create a dropdown to pick a different SAL index\n",
    "salindex_select = Select(\n",
    "    title=\"Inspect Logs (SalIndex):\",\n",
    "    value=str(initial_salindex),\n",
    "    options=[str(idx) for idx in df_executions[\"scriptSalIndex\"]],\n",
    "    width=130,\n",
    ")\n",
    "\n",
    "# When the user changes the dropdown, update the entire table + clear the Div\n",
    "salindex_select.js_on_change(\n",
    "    \"value\",\n",
    "    CustomJS(\n",
    "        args=dict(source=source, data_dict=log_data_by_salindex, tb_div=traceback_div),\n",
    "        code=\"\"\"\n",
    "            const selectedSalIndex = cb_obj.value;\n",
    "            const newData = data_dict[selectedSalIndex] || {\n",
    "                filePath:      [],\n",
    "                functionName:  [],\n",
    "                message:       [],\n",
    "                traceback:     [],\n",
    "                tracebackFull: []\n",
    "            };\n",
    "            source.data = newData;\n",
    "            source.change.emit();\n",
    "            tb_div.text = \"Select a row to see its full traceback here, if the traceback is not empty...\";\n",
    "        \"\"\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 7) When a row is selected, show the FULL traceback in the Div\n",
    "source.selected.js_on_change(\n",
    "    \"indices\",\n",
    "    CustomJS(\n",
    "        args=dict(source=source, tb_div=traceback_div),\n",
    "        code=\"\"\"\n",
    "            const inds = source.selected.indices;\n",
    "            if (inds.length === 0) {\n",
    "                tb_div.text = \"No row selected. Select a row with an existing traceback if you want to expand its content.\";\n",
    "                return;\n",
    "            }\n",
    "            // We take the first selected row for demonstration\n",
    "            const i = inds[0];\n",
    "            const tbFull = source.data.tracebackFull[i];\n",
    "\n",
    "            if (!tbFull || tbFull.trim().length === 0) {\n",
    "                tb_div.text = \"No traceback for this row.\";\n",
    "            } else {\n",
    "                tb_div.text = `<pre>${tbFull}</pre>`;\n",
    "            }\n",
    "        \"\"\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 8) Layout: a column with the dropdown, then table, then the traceback div\n",
    "layout = column(\n",
    "    salindex_select,\n",
    "    row(data_table),\n",
    "    row(traceback_div),\n",
    ")\n",
    "\n",
    "# 9) Show the final app in the notebook\n",
    "show(layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsst-scipipe-9.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
