{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307a6da8-07f5-4271-a211-3bae767b70c5",
   "metadata": {},
   "source": [
    "# Identification of Oscillations using wavelet transforms of M1M3 harpoint forces\n",
    "\n",
    "## Hey this notebook might take O(5 min) to run!\n",
    "\n",
    "Brief summary of this notebook:\n",
    "- A single day_obs is used as the input\n",
    "- For that day_obs a TMA event maker is used to get a set of slews that occured. \n",
    "- For each slew the EFD is queried to get hardpointMeasured forces and az/el telemetry\n",
    "- Then for each hardpoint we compute a wavelet transform and run a peak finder across the resulting frequency vs time array\n",
    "- We search for peaks in frequencies ranging from 5-20 hz that have a power of at least 3000 in the wavelet transform\n",
    "- Next we associate peaks that are clustered in time (and clustered across hardpoints) both with 1-second window\n",
    "- These peaks are returned as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc01500-3166-441e-ac88-e8cc1c78f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times Square parameters\n",
    "\n",
    "day_obs = 20241127\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38424b-048a-4a72-a91e-c90713021468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from lsst.summit.utils.efdUtils import calcNextDay, getEfdData\n",
    "from lsst.summit.utils.tmaUtils import TMAEvent, TMAEventMaker\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bc3a1-737c-4048-a538-eced55e2f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_COUNT = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fa1b1-3933-43b1-b7a9-2173b456e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1M3EFDQuery:\n",
    "    \"\"\"\n",
    "    Query M1M3 telemetry data from the EFD and process it.\n",
    "\n",
    "    This class provides methods for retrieving and analyzing telemetry data\n",
    "    related to the M1M3 Inertia Compensation System during a slew event.\n",
    "\n",
    "    Attributes:\n",
    "        event (TMAEvent): Representation of a slew event.\n",
    "        outer_pad (float): Time padding around the slew event in seconds.\n",
    "        client (EfdClient): EFD client for data retrieval.\n",
    "        number_of_hardpoints (int): Number of hardpoints in the system.\n",
    "        measured_forces_topics (list): Topics for measured forces data.\n",
    "        applied_forces_topics (list): Topics for applied forces data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        event, #TMAEvent,\n",
    "        efd_client, # EfdClient,\n",
    "        outer_pad= 1,\n",
    "    ):\n",
    "\n",
    "        self.event = event\n",
    "        self.outer_pad = outer_pad * u.second\n",
    "        self.client = efd_client\n",
    "\n",
    "        self.number_of_hardpoints = HP_COUNT\n",
    "        self.measured_forces_topics = [\n",
    "            f\"measuredForce{i}\" for i in range(self.number_of_hardpoints)\n",
    "        ]\n",
    "\n",
    "    def query_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query and all relevant telemetry data.\n",
    "\n",
    "        Returns:\n",
    "            dict of dataframes containing telemetry data.\n",
    "        \"\"\"\n",
    "        evt = self.event\n",
    "        query_config = {\n",
    "            \"hp_measured_forces\": {\n",
    "                \"topic\": \"lsst.sal.MTM1M3.hardpointActuatorData\",\n",
    "                \"columns\": self.measured_forces_topics,\n",
    "                \"err_msg\": (\n",
    "                    \"No hard-point data found for event\"\n",
    "                    f\"{evt.seqNum} on {evt.dayObs}\"\n",
    "                ),\n",
    "            },\n",
    "            \"tma_az\": {\n",
    "                \"topic\": \"lsst.sal.MTMount.azimuth\",\n",
    "                \"columns\": [\n",
    "                    \"timestamp\",\n",
    "                    \"actualPosition\",\n",
    "                    \"actualVelocity\",\n",
    "                    \"actualTorque\",\n",
    "                ],\n",
    "                \"err_msg\": (\n",
    "                    \"No TMA azimuth data found for event\"\n",
    "                    f\"{evt.seqNum} on {evt.dayObs}\"\n",
    "                ),\n",
    "                \"reset_index\": True,\n",
    "                \"rename_columns\": {\n",
    "                    \"actualTorque\": \"az_actual_torque\",\n",
    "                    \"actualVelocity\": \"az_actual_velocity\",\n",
    "                    \"actualPosition\": \"az_actual_position\",\n",
    "                },\n",
    "            },\n",
    "            \"tma_el\": {\n",
    "                \"topic\": \"lsst.sal.MTMount.elevation\",\n",
    "                \"columns\": [\n",
    "                    \"timestamp\",\n",
    "                    \"actualPosition\",\n",
    "                    \"actualVelocity\",\n",
    "                    \"actualTorque\",\n",
    "                ],\n",
    "                \"err_msg\": (\n",
    "                    \"No TMA elevation data found for event\"\n",
    "                    f\" {evt.seqNum} on {evt.dayObs}\"\n",
    "                ),\n",
    "                \"reset_index\": True,\n",
    "                \"rename_columns\": {\n",
    "                    \"actualPosition\": \"el_actual_position\",\n",
    "                    \"actualTorque\": \"el_actual_torque\",\n",
    "                    \"actualVelocity\": \"el_actual_velocity\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Query datasets\n",
    "        queries = {\n",
    "            key: self.query_efd_data(**cfg)\n",
    "            for key, cfg in query_config.items()\n",
    "        }  # type: ignore\n",
    "        queries[\"slew\"] = self.event\n",
    "        # Merge datasets\n",
    "        # df = self.merge_datasets(queries)\n",
    "\n",
    "        # Convert torque from Nm to kNm\n",
    "        # cols = [\"az_actual_torque\", \"el_actual_torque\"]\n",
    "        # df.loc[:, cols] *= 1e-3\n",
    "\n",
    "        return queries\n",
    "\n",
    "    def query_efd_data(\n",
    "        self,\n",
    "        topic: str,\n",
    "        columns: list[str],\n",
    "        err_msg: str | None = None,\n",
    "        reset_index: bool = False,\n",
    "        rename_columns: dict | None = None,\n",
    "        resample: float | None = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query telemetry data from the EFD for a given topic.\n",
    "\n",
    "        Parameters:\n",
    "            topic (str): Topic name to query.\n",
    "            columns (list[str]): Columns to retrieve from the topic.\n",
    "            err_msg (str, optional): Error message for missing data.\n",
    "            reset_index (bool, optional): Whether to reset the dataframe index.\n",
    "            rename_columns (dict, optional): Column renaming mapping.\n",
    "            resample (float, optional): Resampling frequency in seconds.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe containing the queried data.\n",
    "        \"\"\"\n",
    "        df = getEfdData(\n",
    "            self.client,\n",
    "            topic,\n",
    "            columns=columns,\n",
    "            event=self.event,\n",
    "            prePadding=self.outer_pad,\n",
    "            postPadding=self.outer_pad,\n",
    "            warn=False,\n",
    "            raiseIfTopicNotInSchema=False,\n",
    "        )\n",
    "        if df.index.size == 0:\n",
    "            # no data return an empty dataframe\n",
    "            begin_timestamp = pd.Timestamp(self.event.begin.unix, unit=\"s\")\n",
    "            end_timestamp = pd.Timestamp(self.event.end.unix, unit=\"s\")\n",
    "            index = pd.DatetimeIndex(\n",
    "                pd.date_range(begin_timestamp, end_timestamp, freq=\"1s\")\n",
    "            )\n",
    "            df = pd.DataFrame(\n",
    "                columns=columns,\n",
    "                index=index,\n",
    "                data=np.zeros((index.size, len(columns))),\n",
    "            )\n",
    "\n",
    "        if rename_columns is not None:\n",
    "            df = df.rename(columns=rename_columns)\n",
    "\n",
    "        if reset_index:\n",
    "            df[\"timestamp\"] = Time(\n",
    "                df[\"timestamp\"], format=\"unix_tai\", scale=\"utc\"\n",
    "            ).datetime\n",
    "            df.set_index(\"timestamp\", inplace=True)\n",
    "            df.index = df.index.tz_localize(\"UTC\")\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904185e-ab9b-41c3-b97b-837127c7d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1M3IdentifyOscillations:\n",
    "    \"\"\"\n",
    "    Identify oscillations in M1M3 telemetry data using wavelet transforms.\n",
    "\n",
    "    Attributes:\n",
    "        day_obs (int): Observation day identifier (YYYYMMDD).\n",
    "        eventMaker (TMAEventMaker): Event maker instance.\n",
    "        events (list[TMAEvent]): List of telemetry events.\n",
    "        slews (list[TMAEvent]): List of slewing events.\n",
    "        peak_height (int): Minimum height of detected peaks.\n",
    "        save_results (bool): Whether to save results as CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, day_obs, peak_height=3000, save_results=True) -> None:\n",
    "        self.day_obs = day_obs\n",
    "        self.eventMaker = TMAEventMaker()\n",
    "        self.events = self.eventMaker.getEvents(self.day_obs)\n",
    "        self.slews = [e for e in self.events]\n",
    "        self.peak_height = peak_height\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run oscillation analysis on all slews for the observation day.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: Dataframe of detected peaks, or\n",
    "            None if no peaks are found.\n",
    "        \"\"\"\n",
    "\n",
    "        peaks_df_list = []\n",
    "        for slew in self.slews:\n",
    "            query_result = M1M3EFDQuery(\n",
    "                slew, self.eventMaker.client, outer_pad=0\n",
    "            ).query_dataset()\n",
    "            peaks_df = self.run_single_slew(\n",
    "                slew.seqNum,\n",
    "                slew.dayObs,\n",
    "                query_result=query_result,\n",
    "                peak_height=self.peak_height,\n",
    "            )\n",
    "\n",
    "            if peaks_df is not None:\n",
    "                peaks_df = peaks_df[peaks_df[\"count\"] >= 2]\n",
    "                if len(peaks_df) > 0:\n",
    "                    print(\n",
    "                        slew.dayObs,\n",
    "                        slew.seqNum,\n",
    "                        f\"found {len(peaks_df)} peak(s)\",\n",
    "                    )\n",
    "                    peaks_df = self.add_telemetry(peaks_df, query_result)\n",
    "                    peaks_df_list.append(peaks_df)\n",
    "            del query_result\n",
    "        if len(peaks_df_list) == 0:\n",
    "            print(f\"No peaks found for {self.day_obs}\")\n",
    "            return None\n",
    "        peaks_df = pd.concat(peaks_df_list, ignore_index=True)\n",
    "\n",
    "        return peaks_df\n",
    "\n",
    "    def identify_peaks_in_wt(\n",
    "        self, coeffs, freqs, times, data, peak_height=1000, time_window=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Identify peaks in the wavelet transform of telemetry data.\n",
    "\n",
    "        Parameters:\n",
    "            coeffs (np.ndarray): Wavelet coefficients.\n",
    "            freqs (np.ndarray): Frequencies corresponding to the coefficients.\n",
    "            times (np.ndarray): Time indices for the data.\n",
    "            data (np.ndarray): Original telemetry data.\n",
    "            peak_height (int): Minimum height for peak detection.\n",
    "            time_window (int): Time tolerance for grouping peaks (seconds).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: Dataframe of detected peaks,\n",
    "            or None if no peaks are found.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute Wavelet Power Spectrum\n",
    "        power = (\n",
    "            np.abs(coeffs) ** 2\n",
    "        )  # Power is the square of the wavelet coefficients\n",
    "\n",
    "        # Detect peaks in the wavelet power spectrum\n",
    "        peaks_time = []\n",
    "        for freq_idx in range(len(freqs)):\n",
    "            peaks, properties = find_peaks(\n",
    "                power[freq_idx], height=peak_height\n",
    "            )  # Adjust `height` for sensitivity\n",
    "            for peak in peaks:\n",
    "                peaks_time.append(\n",
    "                    (\n",
    "                        times[peak],\n",
    "                        freqs[freq_idx],\n",
    "                        properties[\"peak_heights\"][\n",
    "                            np.where(peaks == peak)[0][0]\n",
    "                        ],\n",
    "                    )\n",
    "                )\n",
    "        if len(peaks_time) == 0:\n",
    "            return None\n",
    "        # Convert peaks to a DataFrame for easier processing\n",
    "        peaks_df = pd.DataFrame(\n",
    "            peaks_time, columns=[\"time\", \"frequency\", \"power\"]\n",
    "        )\n",
    "\n",
    "        # Sort by time and group nearby peaks\n",
    "        time_tolerance = timedelta(\n",
    "            seconds=time_window\n",
    "        )  # Tolerance for grouping peaks in seconds\n",
    "        peaks_df = peaks_df.sort_values(\"time\")\n",
    "        grouped_peaks = []\n",
    "        current_group = [peaks_df.iloc[0]]\n",
    "        for i in range(1, len(peaks_df)):\n",
    "            if (\n",
    "                peaks_df.iloc[i][\"time\"] - current_group[-1][\"time\"]\n",
    "                <= time_tolerance\n",
    "            ):\n",
    "                current_group.append(peaks_df.iloc[i])\n",
    "            else:\n",
    "                # Keep only the peak with the maximum power in the group\n",
    "                max_peak = max(current_group, key=lambda x: x[\"power\"])\n",
    "                grouped_peaks.append(max_peak)\n",
    "                current_group = [peaks_df.iloc[i]]\n",
    "\n",
    "        # Add the last group\n",
    "        if current_group:\n",
    "            max_peak = max(current_group, key=lambda x: x[\"power\"])\n",
    "            grouped_peaks.append(max_peak)\n",
    "\n",
    "        # Convert grouped peaks back to a DataFrame\n",
    "        grouped_peaks_df = pd.DataFrame(grouped_peaks)\n",
    "        # remove peaks where data is near zero before\n",
    "        # and near +/- 3k after (breakaway)\n",
    "        grouped_peaks_df = self.remove_breakaway(\n",
    "            grouped_peaks_df, data, time_window=3\n",
    "        )\n",
    "        if len(grouped_peaks_df) == 0:\n",
    "            return None\n",
    "        return grouped_peaks_df\n",
    "\n",
    "    def group_across_hp(self, peaks_df, time_window=1):\n",
    "        \"\"\"\n",
    "        Group peaks across nearby times and retain the one with the\n",
    "        maximum power.\n",
    "        Additionally, count the number of peaks grouped together.\n",
    "\n",
    "        Parameters:\n",
    "        - peaks_df: DataFrame containing 'time' and 'power' columns.\n",
    "        - time_window: Time tolerance (in seconds) for grouping peaks.\n",
    "\n",
    "        Returns:\n",
    "        - grouped_peaks_df: DataFrame with grouped peaks and\n",
    "        counts of grouped occurrences.\n",
    "        \"\"\"\n",
    "        time_tolerance = timedelta(\n",
    "            seconds=time_window\n",
    "        )  # Tolerance for grouping peaks in seconds\n",
    "        peaks_df = peaks_df.sort_values(\"time\")\n",
    "        grouped_peaks = []\n",
    "        current_group = [peaks_df.iloc[0]]\n",
    "\n",
    "        for i in range(1, len(peaks_df)):\n",
    "            if (\n",
    "                peaks_df.iloc[i][\"time\"] - current_group[-1][\"time\"]\n",
    "                <= time_tolerance\n",
    "            ):\n",
    "                current_group.append(peaks_df.iloc[i])\n",
    "            else:\n",
    "                current_group_df = pd.DataFrame(current_group)\n",
    "                max_peak = current_group_df.loc[\n",
    "                    current_group_df[\"power\"].idxmax()\n",
    "                ].copy()\n",
    "                max_peak[\"count\"] = len(\n",
    "                    current_group\n",
    "                )  # Count of peaks in the group\n",
    "                grouped_peaks.append(max_peak)\n",
    "                current_group = [peaks_df.iloc[i]]\n",
    "\n",
    "        # Add the last group\n",
    "        if current_group:\n",
    "            current_group_df = pd.DataFrame(current_group)\n",
    "            max_peak = current_group_df.loc[\n",
    "                current_group_df[\"power\"].idxmax()\n",
    "            ].copy()\n",
    "            max_peak[\"count\"] = len(\n",
    "                current_group\n",
    "            )  # Count of peaks in the last group\n",
    "            grouped_peaks.append(max_peak)\n",
    "\n",
    "        # Convert grouped peaks back to a DataFrame\n",
    "        grouped_peaks_df = pd.DataFrame(grouped_peaks)\n",
    "        return grouped_peaks_df\n",
    "\n",
    "    def run_single_slew(\n",
    "        self,\n",
    "        seq_num,\n",
    "        day_obs,\n",
    "        query_result,\n",
    "        peak_height=3000,\n",
    "        show_plots=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyze oscillations for a single slew.\n",
    "\n",
    "        Parameters:\n",
    "            seq_num (int): Sequence number of the slew.\n",
    "            day_obs (int): Observation day identifier.\n",
    "            query_result (dict): Retrieved telemetry data for the slew.\n",
    "            peak_height (int): Minimum height for peak detection.\n",
    "            show_plots (bool): Whether to show plots of detected peaks.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: Dataframe of detected peaks,\n",
    "            or None if no peaks are found.\n",
    "        \"\"\"\n",
    "        peaks_df_list = []\n",
    "        for hp in range(HP_COUNT):\n",
    "            data = query_result[\"hp_measured_forces\"][f\"measuredForce{hp}\"]\n",
    "            times = query_result[\"hp_measured_forces\"].index.values\n",
    "            coeffs, freqs = self.compute_wt(data)\n",
    "            peaks_df = self.identify_peaks_in_wt(\n",
    "                coeffs, freqs, times, data, peak_height=peak_height\n",
    "            )\n",
    "            if peaks_df is None:\n",
    "                continue\n",
    "            if show_plots:\n",
    "                self.plot_results_wt(\n",
    "                    coeffs, times, freqs, peaks_df, hp, seq_num, day_obs\n",
    "                )\n",
    "            peaks_df[\"seq_num\"] = seq_num\n",
    "            peaks_df[\"max_power_hp_num\"] = hp\n",
    "            peaks_df[\"day_obs\"] = day_obs\n",
    "            peaks_df_list.append(peaks_df)\n",
    "\n",
    "        if len(peaks_df_list) == 0 | (np.all(peaks_df_list is None)):\n",
    "            return None\n",
    "\n",
    "        peaks_df = pd.concat(peaks_df_list, ignore_index=True)\n",
    "\n",
    "        peaks_df = self.group_across_hp(peaks_df)\n",
    "\n",
    "        return peaks_df\n",
    "\n",
    "    def add_telemetry(self, peaks_df, query_result):\n",
    "        \"\"\"\n",
    "        Add telemetry data to the detected peaks dataframe.\n",
    "\n",
    "        Parameters:\n",
    "            peaks_df (pd.DataFrame): Dataframe of detected peaks.\n",
    "            query_result (dict): Retrieved telemetry data for the slew.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataframe with added telemetry information.\n",
    "        \"\"\"\n",
    "        cols = []\n",
    "        for mt_ax in [\"az\", \"el\"]:\n",
    "            cols += [\n",
    "                f\"{mt_ax}_actual_position\",\n",
    "                f\"{mt_ax}_actual_velocity\",\n",
    "                f\"{mt_ax}_actual_torque\",\n",
    "            ]\n",
    "        telem_dict = {key: [] for key in cols}\n",
    "\n",
    "        for i, row in peaks_df.iterrows():\n",
    "            t0 = Time(row[\"time\"], scale=\"utc\").datetime64\n",
    "            for mt_ax in [\"az\", \"el\"]:\n",
    "                dat = query_result[f\"tma_{mt_ax}\"]\n",
    "                idxmin = np.argmin(abs(dat.index.values - t0))\n",
    "                for key in [\n",
    "                    f\"{mt_ax}_actual_position\",\n",
    "                    f\"{mt_ax}_actual_velocity\",\n",
    "                    f\"{mt_ax}_actual_torque\",\n",
    "                ]:\n",
    "                    telem_dict[key].append(dat[key].iloc[idxmin])\n",
    "        telem_df = pd.DataFrame(telem_dict)\n",
    "        peaks_df = pd.concat(\n",
    "            [peaks_df.reset_index(drop=True), telem_df.reset_index(drop=True)],\n",
    "            axis=1,\n",
    "        )\n",
    "        return peaks_df\n",
    "\n",
    "    def remove_breakaway(self, peaks_df, data, time_window=3):\n",
    "        \"\"\"\n",
    "        Remove breakaway peaks from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "            peaks_df (pd.DataFrame): Dataframe of detected peaks.\n",
    "            data (pd.Series): Original telemetry data.\n",
    "            time_window (int): Time window for breakaway detection (seconds).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered dataframe without breakaway peaks.\n",
    "        \"\"\"\n",
    "        # returns false if there is a breakaway\n",
    "        time_window = timedelta(seconds=time_window)\n",
    "        breakaway_sel = []\n",
    "        for peak_time in peaks_df[\"time\"]:\n",
    "            sel_before = (data.index.values < peak_time) & (\n",
    "                data.index.values > peak_time - time_window\n",
    "            )\n",
    "            sel_after = (data.index.values > peak_time) & (\n",
    "                data.index.values < peak_time + time_window\n",
    "            )\n",
    "            median_before = np.median(data[sel_before])\n",
    "            median_after = np.median(data[sel_after])\n",
    "            if (abs(median_before) < 10) & (abs(median_after) > 2000):\n",
    "                breakaway_sel.append(False)\n",
    "                print(f\"breakaway detected removing {peak_time}\")\n",
    "            else:\n",
    "                breakaway_sel.append(True)\n",
    "\n",
    "        return peaks_df[breakaway_sel].reset_index(drop=True)\n",
    "\n",
    "    def plot_results_wt(\n",
    "        self, coeffs, times, freqs, grouped_peaks_df, hp, seq_num, day_obs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot wavelet transform results and detected peaks.\n",
    "\n",
    "        Parameters:\n",
    "            coeffs (np.ndarray): Wavelet coefficients.\n",
    "            times (np.ndarray): Time indices for the data.\n",
    "            freqs (np.ndarray): Frequencies corresponding to the coefficients.\n",
    "            grouped_peaks_df (pd.DataFrame): Dataframe of grouped peaks.\n",
    "            hp (int): Hardpoint identifier.\n",
    "            seq_num (int): Sequence number of the slew.\n",
    "            day_obs (int): Observation day identifier.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        power = np.abs(coeffs) ** 2\n",
    "        plt.imshow(\n",
    "            power,\n",
    "            extent=[times.min(), times.max(), freqs.max(), freqs.min()],\n",
    "            aspect=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "            vmax=1000,\n",
    "        )\n",
    "        plt.colorbar(label=\"Wavelet Power\")\n",
    "        plt.ylabel(\"Frequency (Hz)\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.title(f\"measuredForce{hp}, seq_num:{seq_num}, day_obs:{day_obs}\")\n",
    "\n",
    "        # Plot detected peaks (grouped)\n",
    "        plt.scatter(\n",
    "            grouped_peaks_df[\"time\"],\n",
    "            grouped_peaks_df[\"frequency\"],\n",
    "            color=\"red\",\n",
    "            s=20,\n",
    "            label=\"Grouped Peaks\",\n",
    "        )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_wt(\n",
    "        data,\n",
    "        sampling_period=0.02,\n",
    "        frequency_scales=np.arange(5, 20, 0.2),\n",
    "        wavelet=\"cmor1.5-1.0\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the wavelet transform of the telemetry data.\n",
    "\n",
    "        Parameters:\n",
    "            data (np.ndarray): Input telemetry data.\n",
    "            sampling_period (float): Sampling period in seconds.\n",
    "            frequency_scales (np.ndarray): Frequency scales for the\n",
    "            wavelet transform.\n",
    "            wavelet (str): Wavelet type.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.ndarray, np.ndarray]: Wavelet coefficients\n",
    "            and corresponding frequencies.\n",
    "        \"\"\"\n",
    "\n",
    "        scales = pywt.frequency2scale(\n",
    "            wavelet, frequency_scales * sampling_period\n",
    "        )\n",
    "        coeffs, freqs = pywt.cwt(\n",
    "            data,\n",
    "            scales=scales,\n",
    "            wavelet=\"cmor1.5-1.0\",\n",
    "            sampling_period=sampling_period,\n",
    "        )\n",
    "        return coeffs, freqs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display_pretty(df, decimal_places=2):\n",
    "    \"\"\"\n",
    "    Format numeric columns in a DataFrame to a specified number of decimal places\n",
    "    while leaving non-numeric columns unchanged.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        decimal_places (int): The number of decimal places for numeric columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.io.formats.style.Styler: A styled DataFrame with numeric columns formatted.\n",
    "    \"\"\"\n",
    "    # Create a format dictionary for numeric columns\n",
    "    format_dict = {}\n",
    "    for col in df.select_dtypes(include=\"float64\").columns:\n",
    "        format_dict[col] = f\"{{:.{decimal_places}f}}\"\n",
    "    format_dict['time'] = lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if pd.notnull(x) else \"\"\n",
    "    \n",
    "    # Apply formatting to the DataFrame\n",
    "    return df.style.format(format_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cada0c3-33f2-4f87-b521-c929b914e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = M1M3IdentifyOscillations(day_obs)\n",
    "result_df = analysis.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904223e7-4f4e-4e69-a86f-8d271516a2cb",
   "metadata": {},
   "source": [
    "## Likely candidates\n",
    "- az glitches usually have a frequency > 14 hz\n",
    "- lots of non osillation bumps will be flagged with frequency < 8\n",
    "- Also if count (number of hardpoints that showed an oscillation) is less than 4 than candidate is less interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80a483-f286-4ca9-ad1c-50396155cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = (result_df[\"frequency\"] > 8) | (result_df[\"power\"] > 10000)\n",
    "display_pretty(result_df[sel])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cf26c-9669-4580-8511-5cf59bb6f70b",
   "metadata": {},
   "source": [
    "# Other Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a40cb8-998e-4d4a-aa47-f0ff01e1e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pretty(result_df[~sel])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
